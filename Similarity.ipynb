{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc61ad-829b-4459-8593-1cb28a24702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d7ee10-f190-45e7-a772-24e818303f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = \"/home/shubham4413/GSOC/Similarity/data/\"\n",
    "\n",
    "SIMILARITY_MODEL  = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(SIMILARITY_MODEL)\n",
    "bert_model = AutoModel.from_pretrained(SIMILARITY_MODEL)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    #Tokenizing text and converting to tensors\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "    return tokens\n",
    "\n",
    "def detect_column_types(df):\n",
    "    #Automatically classifying columns as numerical, categorical, or text.\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    text_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].nunique()\n",
    "        total_values = len(df[col])\n",
    "\n",
    "        if df[col].dtype in ['int64', 'float64']:  # Numeric Data\n",
    "            numerical_cols.append(col)\n",
    "\n",
    "        elif df[col].dtype == 'object':  # Object/String Data\n",
    "            avg_length = df[col].astype(str).apply(len).mean()\n",
    "\n",
    "            if unique_values / total_values < 0.2:  # Few unique values → Categorical\n",
    "                categorical_cols.append(col)\n",
    "            elif avg_length > 20:  # Long text → Treat as Text Column\n",
    "                text_cols.append(col)\n",
    "            else:  # Short text but many unique values → Treat as Categorical\n",
    "                categorical_cols.append(col)\n",
    "\n",
    "    return numerical_cols, categorical_cols, text_cols\n",
    "\n",
    "def encode_data(df, numerical_cols, categorical_cols, text_cols):\n",
    "    # Normalize numerical columns\n",
    "  if numerical_cols:\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    # Encode categorical columns\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le \n",
    "\n",
    "    # Convert text columns to BERT embeddings\n",
    "    def extract_bert_embedding(text):\n",
    "        tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = bert_model(**tokens)\n",
    "        return output.last_hidden_state[:, 0, :].squeeze().numpy()  # Extract CLS token\n",
    "\n",
    "    if text_cols:\n",
    "        for col in text_cols:\n",
    "            df[col + \"_bert\"] = df[col].astype(str).apply(lambda x: extract_bert_embedding(x))\n",
    "            df.drop(columns=[col], inplace=True)  # Drop original text column after encoding\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4668436-e11a-492d-bce5-0a7d0f263013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: objects.csv\n",
      "Detected -> Numerical: ['objectid', 'accessioned', 'locationid', 'beginyear', 'endyear', 'parentid', 'isvirtual'], Categorical: ['accessionnum', 'displaydate', 'visualbrowsertimespan', 'medium', 'markings', 'attributioninverted', 'attribution', 'provenancetext', 'creditline', 'classification', 'subclassification', 'visualbrowserclassification', 'departmentabbr', 'portfolio', 'series', 'volume', 'watermarks', 'lastdetectedmodification', 'wikidataid', 'customprinturl'], Text: ['title', 'dimensions', 'inscription']\n"
     ]
    }
   ],
   "source": [
    "processed_data = {}\n",
    "\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        print(f\"Processing: {file}\")\n",
    "\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        numerical_cols, categorical_cols, text_cols = detect_column_types(df)\n",
    "        print(f\"Detected -> Numerical: {numerical_cols}, Categorical: {categorical_cols}, Text: {text_cols}\")\n",
    "\n",
    "        df_processed = encode_data(df, numerical_cols, categorical_cols, text_cols)\n",
    "        processed_data[file] = df_processed  # Store processed DataFrame\n",
    "\n",
    "        # Save processed file\n",
    "        df_processed.to_pickle(os.path.join(csv_folder, file.replace(\".csv\", \"_processed.pkl\")))\n",
    "\n",
    "print(\"All files processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395142bc-06df-4612-9b98-5367df68aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convering the processed data into dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, text_column, numerical_columns, tokenizer):\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Tokenize text\n",
    "        text_data = self.tokenizer(row[self.text_column], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Extract numerical features\n",
    "        numerical_data = torch.tensor(row[self.numerical_columns].values, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_data[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_data[\"attention_mask\"].squeeze(0),\n",
    "            \"numerical_data\": numerical_data\n",
    "        }\n",
    "\n",
    "dataset = CustomDataset(processed_data, text_column=\"text_column\", numerical_columns=numerical_columns, tokenizer=tokenizer)\n",
    "batch_size = 32 \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86592088-f683-4838-9b28-2f697071aeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimilarityModel(\n",
      "  (bert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fusion): Linear(in_features=832, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimilarityModel(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"distilbert-base-uncased\", numerical_input_dim=10):\n",
    "        super(SimilarityModel, self).__init__()\n",
    "        \n",
    "        # Loading pretrained BERT model\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.bert_output_dim = self.bert.config.hidden_size  #768 for BERT\n",
    "        \n",
    "        # MLP for numerical data\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(numerical_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fusion layer (BERT + MLP outputs)\n",
    "        self.fusion = nn.Linear(self.bert_output_dim + 64, 128)\n",
    "        \n",
    "        # Output similarity embedding\n",
    "        self.output_layer = nn.Linear(128, 64)  # Output size for similarity comparison\n",
    "\n",
    "    def forward(self, text_inputs, numerical_inputs):\n",
    "        # BERT embedding for text data\n",
    "        bert_outputs = self.bert(**text_inputs)\n",
    "        text_embedding = bert_outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "\n",
    "        # MLP embedding for numerical data\n",
    "        numerical_embedding = self.mlp(numerical_inputs)\n",
    "\n",
    "        # Concatenate text and numerical embeddings\n",
    "        combined_embedding = torch.cat((text_embedding, numerical_embedding), dim=1)\n",
    "\n",
    "        # Fusion layer\n",
    "        fused_output = F.relu(self.fusion(combined_embedding))\n",
    "\n",
    "        # Final embedding for similarity\n",
    "        final_embedding = self.output_layer(fused_output)\n",
    "        return final_embedding\n",
    "\n",
    "# Example usage\n",
    "model = SimilarityModel(numerical_input_dim=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7e66fc-c406-421c-9200-4c4ce1841602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_loss(embedding1, embedding2, labels, margin=0.5):\n",
    "    # Computing contrastive loss based on cosine similarity.\n",
    "    similarity = F.cosine_similarity(embedding1, embedding2)\n",
    "    loss = labels * (1 - similarity) + (1 - labels) * torch.clamp(similarity - margin, min=0.0)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf98d2b-09e5-4f4c-9d73-f40bfed138d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = cosine_similarity_loss\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader: \n",
    "        text_inputs, numerical_inputs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        embedding1 = model(text_inputs[0], numerical_inputs[0])\n",
    "        embedding2 = model(text_inputs[1], numerical_inputs[1])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(embedding1, embedding2, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bfbcc-6885-4156-a45f-37b661321eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "text_inputs, numerical_inputs, labels = batch\n",
    "\n",
    "embedding1 = model(text_inputs[0], numerical_inputs[0])\n",
    "embedding2 = model(text_inputs[1], numerical_inputs[1])\n",
    "\n",
    "# 1) Compute Cosine Similarity\n",
    "cos_sim = F.cosine_similarity(embedding1, embedding1)\n",
    "\n",
    "# 2) Compute Euclidean Distance\n",
    "euclidean_dist = torch.dist(embedding1, embedding2, p=2)\n",
    "\n",
    "# 3) Contrastive loss\n",
    "magin = 1.0\n",
    "c_loss = (1 - label) * torch.pow(euclidean_dist, 2) + label * torch.pow(torch.clamp(margin - euclidean_dist, min=0.0), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
